{"name":"Webcam-pulse-detector","tagline":"A python application that detects and highlights the heart-rate of an individual (using only their own webcam) in real-time.","body":"![Alt text](http://i.imgur.com/2ngZopS.jpg \"Screenshot\")\r\n\r\nwebcam-pulse-detector\r\n-----------------------\r\n\r\nA python code that detects the heart-rate of an individual using a common webcam or network IP camera. \r\nTested on OSX 10.8 (Mountain Lion), Ubuntu 13.04 (Ringtail), and Windows 7 & 8.\r\n\r\nInspired by reviewing recent work on [Eulerian Video Magnification](http://people.csail.mit.edu/mrub/vidmag/), \r\nwith motivation to implement something visually comparable (though not necessarily identical in formulation) to their\r\npulse detection examples using [Python](http://python.org/) and [OpenCV](http://opencv.org/) (see https://github.com/brycedrennan/eulerian-magnification for a \r\nmore general take on the offline post-processing methodology). \r\nThis goal is comparable to those of a few previous efforts in this area \r\n(such as https://github.com/mossblaser/HeartMonitor).\r\n\r\nThis code was developed at [NASA Glenn Research Center](http://www.nasa.gov/centers/glenn) in \r\nsupport of [OpenMDAO](http://openmdao.org/), under the Aeronautical Sciences Project in NASA's \r\n[Fundamental Aeronautics Program](http://www.aeronautics.nasa.gov/fap/), as well as the Crew State Monitoring Element \r\nof the Vehicle Systems Safety Technologies Project, in NASAâ€™s \r\n[Aviation Safety Program](http://www.aeronautics.nasa.gov/programs_avsafe.htm).\r\n\r\nA list of other open-source NASA codes can be found at [code.nasa.gov](http://code.nasa.gov/project/).\r\n\r\nHow it works:\r\n-----------------\r\nThis application uses [OpenCV](http://opencv.org/) to find the location of the user's face, then isolate the forehead region. Data is collected\r\nfrom this location over time to estimate the user's heart rate. This is done by measuring average optical\r\nintensity in the forehead location, in the subimage's green channel alone (a better color mixing ratio may exist, but the \r\nblue channel tends to be very noisy). Physiological data can be estimated this way thanks to the optical absorption \r\ncharacteristics of (oxy-) haemoglobin (see http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-16-26-21434). \r\n\r\nWith good lighting and minimal noise due to motion, a stable heartbeat should be \r\nisolated in about 15 seconds. Other physiological waveforms (such as \r\n[Mayer waves](http://en.wikipedia.org/wiki/Mayer_waves)) should also be visible in the raw data stream.\r\n\r\nOnce the user's heart rate has been estimated, real-time phase variation associated with this \r\nfrequency is also computed. This allows for the heartbeat to be exaggerated in the post-process frame rendering, \r\ncausing the highlighted forehead location to pulse in sync with the user's own heartbeat.\r\n\r\nSupport for detection on multiple simultaneous individuals in a single camera's \r\nimage stream is definitely possible, but at the moment only the information from one face \r\nis extracted for analysis.\r\n\r\nThe overall dataflow/execution order for the real-time signal processing looks like:\r\n\r\n![Alt text](http://i.imgur.com/xS7O8U3.png \"Signal processing\")\r\n\r\nThis signal processing design is implemented in the openMDAO assembly object defined in\r\n[lib/processors.py](lib/processors.py).\r\n\r\nThe definition of each component block used can be found in the source \r\nfiles [lib/imageProcess.py](lib/imageProcess.py), [lib/signalProcess.py](lib/signalProcess.py), and \r\n[lib/sliceops.py](lib/sliceops.py). The `@bin` and `@bout` blocks in the above graph denote assembly-level input and \r\noutput.\r\n\r\n\r\nRequirements:\r\n---------------\r\n\r\n- [Python v2.7+](http://python.org/)\r\n- [OpenCV v2.4+](http://opencv.org/), with the cv2 python bindings\r\n \r\nOpenCV is a powerful open-source computer vision library, with a convenient \r\nnumpy-compatible interface in the cv2 bindings.\r\n\r\n- [OpenMDAO v0.5.5+](http://openmdao.org/)\r\n\r\nOpenMDAO is an open-source engineering framework that serves as a convenient \r\nenvironment to containerize the required real-time analysis, and \r\nallow for that analysis to be easily tweaked to specification and compared with alternative designs. \r\nUpon installation, OpenMDAO is bootstrapped into its own Python \r\nvirtualenv, which must be activated before use (see the Quickstart section below). OpenMDAO requires python 2.6+, numpy, scipy, and matplotlib \r\n(see http://openmdao.org/docs/getting-started/requirements.html)\r\n\r\nRunning Windows, and completely new to Python? Full instructions for getting started with all requirements needed to\r\nrun this code are available [here](win_pythonxy.md)\r\n\r\nQuickstart:\r\n------------\r\n- Activate the openMDAO virtual python environment in a command or terminal window. On Linux and OSX, this is done by\r\nrunning (note the period):\r\n\r\n```\r\n. OpenMDAO/bin/activate\r\n```\r\nOr on Windows:\r\n\r\n```\r\nOpenMDAO\\Scripts\\activate\r\n```\r\n\r\n- In the activated environment, navigate to the downloaded source directory, and run get_pulse.py to start the application\r\n\r\n```\r\npython get_pulse.py\r\n```\r\n\r\n- To run on an IP camera, set the `url`, `user`, and `password` strings on line 134 of `get_pulse_ipcam.py`, then run:\r\n\r\n```\r\npython get_pulse_ipcam.py\r\n```\r\nThis was tested on a Wowwee Rovio.\r\n\r\n- If there is an error, try running `test_webcam.py` in the same directory to check if your openCV installation and webcam can be made to work\r\nwith this application.\r\n\r\nUsage notes:\r\n----------\r\n- When run, a window will open showing a stream from your computer's webcam\r\n- When a forehead location has been isolated, the user should press \"S\" on their \r\nkeyboard to lock this location, and remain as still as possible (the camera \r\nstream window must have focus for the click to register). This freezes the acquisition location in place. This lock can\r\nbe released by pressing \"S\" again.\r\n- To view a stream of the measured data as it is gathered, press \"D\". To hide this display, press \"D\" again.\r\n- The data display shows three data traces, from top to bottom: \r\n   1. raw optical intensity\r\n   2. extracted heartbeat signal\r\n   3. Power spectral density, with local maxima indicating the heartrate (in beats per minute). \r\n- With consistent lighting and minimal head motion, a stable heartbeat should be \r\nisolated in about 15 to 20 seconds. A count-down is shown in the image frame.\r\n- If a large spike in optical intensity is measured in the data (due to motion \r\nnoise, sudden change in lighting, etc) the data collection process is reset and \r\nstarted over. The sensitivity of this feature can be tweaked by changing `data_spike_limit` on line 31 of [get_pulse.py](get_pulse.py).\r\nOther mutable parameters of the analysis can be changed here as well.\r\n\r\nTODO:\r\n------\r\n- There have been some requests for a video demo\r\n- Instead of processing using the green channel alone, it is likely that some fixed combination of the statistics of the\r\nR,G,B channels could instead be optimal (though I was unable to find a simple combination that was better than green\r\nalone). If so, the mixing ratios might be determinable from the forward projection matrices of PCA or ICA operators \r\ncomputed on a set of mean value R,G, and B data gathered over a trial data set (and verified with different individuals \r\nunder different lighting conditions).\r\n- Support for multiple individuals\r\n- Smoother tracking of data from foreheads, perhaps by buffering and registering/inverse-transforming image subframes\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}